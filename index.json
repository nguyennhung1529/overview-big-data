[
{
	"uri": "//localhost:1313/",
	"title": "Big Data Tutorial: Overview về BigData",
	"tags": [],
	"description": "",
	"content": "Overview về BigData Trong tài liệu này mình sẽ giới thiệu sơ qua cho mọi người nắm được các kiến thức cơ bản nhất cần chuẩn bị trước khi bắt đầu nghiên cứu thực hiện một dự án về big data.\nNội dung Tổng quan về Big Data và Ecosystem Lưu trữ, quản lý và xử lý Big Data "
},
{
	"uri": "//localhost:1313/2-etl-in-bigdata/2.1-data-models/",
	"title": "Data Models",
	"tags": [],
	"description": "",
	"content": "Trong lĩnh vực data, mô hình dữ liệu (data model) đóng vai trò quan trọng trong việc tổ chức, lưu trữ và quản lý dữ liệu. Có nhiều loại mô hình dữ liệu, nhưng một số phổ biến bao gồm:\n1.Mô hình quan hệ (Relational Model) Được sử dụng rộng rãi trong các hệ thống quản trị cơ sở dữ liệu quan hệ (RDBMS). Dữ liệu được tổ chức thành các bảng, với mỗi bảng gồm hàng (dữ liệu) và cột (thuộc tính). Các bảng có thể liên kết với nhau thông qua các khóa. 2. Mô hình dữ liệu phi quan hệ (NoSQL) Thích hợp cho dữ liệu không cấu trúc hoặc bán cấu trúc, và khi cần khả năng mở rộng ngang. Bao gồm một số loại như: key-value store, document store, wide-column store, và graph databases. Ví dụ: MongoDB / AWS DynamoDB (document store), Cassandra (wide-column store), Neo4j / Amazon Neptune (graph database). 3. Mô hình đa chiều (Multidimensional Model) Thường được sử dụng trong data warehousing và business intelligence (BI) để hỗ trợ truy vấn và phân tích dữ liệu nhanh chóng. Dữ liệu được tổ chức thành các cube, với mỗi chiều (dimension) đại diện cho một thuộc tính dữ liệu, và các cell trong cube chứa các giá trị dữ liệu. 4. Mô hình đồ thị (Graph Model) Sử dụng đồ thị (nodes và edges) để mô hình hóa dữ liệu và mối quan hệ giữa chúng. Phù hợp với các ứng dụng mà mối quan hệ giữa các đối tượng là quan trọng, như mạng xã hội, hệ thống đề xuất, và phân tích liên kết. Trong Big Data, do tính chất đa dạng và lớn về khối lượng dữ liệu, tùy thuộc vào yêu cầu cụ thể của dự án mà có thể sẽ thường chọn các loại data models sau:\nMô hình phi quan hệ (NoSQL): Do khả năng mở rộng ngang và linh hoạt trong việc lưu trữ dữ liệu không cấu trúc hoặc bán cấu trúc, NoSQL thường được yêu thích lựa chọn trong các ứng dụng Big Data. Mô hình đồ thị (Graph Model): Dành cho các dự án liên quan đến phân tích mạng xã hội, đề xuất nội dung, hoặc các ứng dụng cần đến việc hiểu rõ mối quan hệ giữa các đối tượng. "
},
{
	"uri": "//localhost:1313/1-overview-bigdata-and-ecosystem/",
	"title": "Tổng quan về Big Data và Ecosystem",
	"tags": [],
	"description": "",
	"content": "Big Data là thuật ngữ dùng để mô tả khối lượng dữ liệu rất lớn, phức tạp tới mức các công cụ xử lý dữ liệu truyền thống không có khả năng thu thập, lưu trữ, quản lý và phân tích/xử lý trong một khoảng thời gian hợp lý. Big Data không chỉ đơn giản về khối lượng dữ liệu lớn mà còn về tốc độ dữ liệu được tạo ra và đa dạng của dữ liệu đó (có cấu trúc, không cấu trúc và bán cấu trúc).\nVí dụ về ứng dụng của Big Data trong lĩnh vực ngân hàng như phân tích rủi ro và gian lận, tối ưu hóa danh mục đầu tư từ lượng lớn dữ liệu thu thập từ thị trường tài chính, … Hoặc trong lĩnh vực marketing có ứng dụng sử lý bigdata cho việc nhận diện hành vi khách hàng giúp cá nhân hóa dịch vụ và sản phẩm, hoặc ơhân tích dữ liệu từ nhiều nguồn để tối ưu chiến dịch quảng cáo, …. Ngoài ra còn nhiều ngành đang áp dụng Big Data như nông nghiệp, giáo dục… =\u0026gt; cung cấp insight tốt về dữ liệu =\u0026gt; hỗ trợ ra quyết định nhanh chóng và chính xác.\nBig Data thường được mô tả thông qua ba \u0026ldquo;V\u0026rdquo;:\nVolume (Khối lượng): Lượng dữ liệu lớn, thường là petabytes hoặc exabytes được tạo ra từ các nguồn khác nhau như doanh nghiệp, máy móc, thiết bị IoT, truyền thông xã hội, v.v. Velocity (Tốc độ): Tốc độ nhanh chóng mà dữ liệu được tạo ra và cần phải được thu thập, xử lý. Trong nhiều trường hợp, dữ liệu cần được xử lý nhanh chóng để khai thác sử dụng. Variety (Đa dạng): Sự đa dạng về loại dữ liệu, bao gồm dữ liệu cấu trúc (Excel, dữ liệu trong các bảng trong DB, …), không cấu trúc (hình ảnh, video, pdf, …) và bán cấu trúc (XML, HTLM, …). Ngoài ra, hai \u0026ldquo;V\u0026rdquo; khác cũng thường được đề cập:\nVeracity (Độ chính xác): Độ tin cậy và chính xác của dữ liệu (có thể bị ảnh hưởng bởi dữ liệu nhiễu, dữ liệu không đầy đủ hoặc lỗi). Value (Giá trị): Khả năng trích xuất thông tin có ích và có giá trị từ dữ liệu. Bao nhiêu dữ liệu mới đủ gọi là \u0026lsquo;big\u0026rsquo;?\nKhông có một ngưỡng cụ thể nào vì nó phụ thuộc vào khả năng khai thác và xử lý của công nghệ hiện tại và nhu cầu cụ thể của mỗi tổ chức. Một lượng dữ liệu có thể được coi là \u0026ldquo;lớn\u0026rdquo; ở thời điểm này có thể không còn phù hợp trong vài năm sau do sự phát triển của công nghệ và kỹ thuật xử lý dữ liệu. Tuy nhiên, một cách tổng quát, khi dữ liệu bắt đầu tính bằng petabytes trở lên và không thể được xử lý một cách hiệu quả bằng cơ sở dữ liệu truyền thống, nó thường được coi là Big Data.\n"
},
{
	"uri": "//localhost:1313/2-etl-in-bigdata/2.2-storage-system/",
	"title": "Hệ thống lưu trữ",
	"tags": [],
	"description": "",
	"content": "Việc chọn loại hệ thống lưu trữ sẽ phụ thuộc vào data models, lựa chọn hệ thống lưu trữ phù hợp là yếu tố quan trọng để đảm bảo hiệu suất và khả năng mở rộng, đặc biệt là đối với Big Data. Dưới đây là một số loại hệ thống lưu trữ Big Data phổ biến:\nHệ thống lưu trữ phân tán (HDFS): Đặc điểm: Là hệ thống lưu trữ dựa trên mô hình hệ thống file phân tán được thiết kế để chạy trên phần cứng thông thường. HDFS có khả năng lưu trữ dữ liệu với khối lượng lớn và hỗ trợ việc sao chép và phân tán dữ liệu trên nhiều máy để đảm bảo độ tin cậy và khả năng phục hồi. Ứng dụng: Phù hợp cho việc lưu trữ và xử lý dữ liệu không cấu trúc hoặc bán cấu trúc với khối lượng lớn. NoSQL Databases Đặc điểm: Là loại cơ sở dữ liệu không tuân thủ theo mô hình cơ sở dữ liệu quan hệ truyền thống, được thiết kế để lưu trữ dữ liệu không cấu trúc hoặc bán cấu trúc với khả năng mở rộng cao. Ví dụ: MongoDB, Cassandra, HBase, Couchbase, AWS DynamoDB, \u0026hellip;. Ứng dụng: Thích hợp cho các ứng dụng cần độ mở rộng cao và thời gian phản hồi nhanh, như web và ứng dụng di động, dữ liệu thời gian thực và IoT. Data Lakes: Cung cấp khả năng lưu trữ dữ liệu thô ở quy mô lớn, bao gồm dữ liệu cấu trúc và không cấu trúc; Data lake này có thể là một hệ thống lưu trữ on-prem hoặc một dịch vụ Data Lake on-cloud. Ví dụ: Hệ thống data lake on-premises, AWS S3, Google Cloud Storage, Azure Data Lake Storage, \u0026hellip; Bên cạnh các loại hệ thống lưu trữ trên, còn có hệ thống lưu trữ điển hình khác đó là Data Warehouses - Cho phép lưu trữ dữ liệu đã được cấu trúc hóa và chuẩn hóa, thích hợp cho việc phân tích và báo cáo. Nếu Data Lake phù hợp với việc lưu trữ và phân tích dữ liệu Big Data thô và không cấu trúc (dữ liệu input); thì Data Warehouse tối ưu cho việc lưu trữ, truy vấn, và phân tích dữ liệu đã được cấu trúc hóa (output dữ liệu sau khi đã được tranform thành các tập có cấu trúc).\nPhần này mình sẽ không giới thiệu về Data Lakehouse, vì hiện nay Data Lakehouse vẫn còn là một khái niệm tương đối mới và đang phát triển, vì vậy các công nghệ và công cụ hỗ trợ còn đang trong quá trình hoàn thiện =\u0026gt; rủi ro và thách thức trong việc triển khai và bảo trì. Tuy nhiên, sơ lược thì Data Lakehouse = Data Lake + Data Warehouse. Lấy ưu điểm của cả Data Lake và Data Warehouse, cung cấp cả khả năng lưu trữ dữ liệu thô lớn của Data Lake và tính năng quản lý và truy vấn dữ liệu cấu trúc hóa của Data Warehouse.\n"
},
{
	"uri": "//localhost:1313/2-etl-in-bigdata/",
	"title": "Lưu trữ, quản lý và xử lý Big Data",
	"tags": [],
	"description": "",
	"content": "\nNội dung 2.1. Data Models 2.2. Hệ thống lưu trữ 2.3. ETL trong Big data\n"
},
{
	"uri": "//localhost:1313/2-etl-in-bigdata/2.3-etl-in-bigdata/",
	"title": "ETL trong Big data",
	"tags": [],
	"description": "",
	"content": "ETL ETL, viết tắt của Extract, Transform, Load, là một quy trình quan trọng trong việc xử lý và chuẩn bị dữ liệu, đặc biệt đối với Big Data giúp tối ưu hóa dữ liệu cho phân tích, đảm bảo dữ liệu đầu vào đủ chất lượng cho quá trình phân tích về sau. Trong đó: Extract (Trích xuất): Dữ liệu được trích xuất từ một hoặc nhiều nguồn dữ liệu, có thể gồm cơ sở dữ liệu truyền thống, hệ thống tệp, hoặc từ các thiết bị IoT, ….. Là bước đầu tiên và cơ bản nhất, yêu cầu tính chính xác cao để đảm bảo dữ liệu được thu thập đầy đủ và không bị biến đổi. Transform (Biến đổi): Dữ liệu trích xuất sau đó được làm sạch (loại bỏ dữ liệu nhiễu), chuẩn hóa, biến đổi, và lọc để đáp ứng nhu cầu và tiêu chuẩn cụ thể của dự án hoặc tổ chức. Load (Tải): Dữ liệu sau khi được biến đổi sẽ được tải vào kho dữ liệu hoặc hệ thống lưu trữ cuối, có thể là một Data Warehouse, Data Lake, hoặc Database phục vụ cho việc truy vấn phân tích về sau. ETL giúp tối ưu hóa dữ liệu cho phân tích, đảm bảo dữ liệu đầu vào đủ chất lượng cho quá trình phân tích.\nBatch data \u0026amp; Streaming Data Batch Data và Streaming Data là hai hướng tiếp cận chính trong việc xử lý và phân tích dữ liệu, sơ lược qua về 2 loại xử lý dữ liệu này:\nBatch Data Processing (Xử lý dữ liệu theo lô) Khái niệm: Là phương pháp mà dữ liệu được thu thập, lưu trữ và xử lý theo các lô (batch) hoặc đợt cố định (ví dụ theo lập lịch). Dữ liệu được xử lý trong một khoảng thời gian nhất định và thường đòi hỏi một lượng lớn dữ liệu để bắt đầu quá trình xử lý. Ứng dụng: Thích hợp cho các tác vụ phức tạp cần xử lý dữ liệu lớn. (VD: phân tích kinh doanh, báo cáo, và các quy trình ETL truyền thống). Ví dụ: Tính toán metrics kinh doanh hàng ngày, xử lý các giao dịch cuối ngày trong ngành ngân hàng, hoặc phân tích log truy cập website hàng tháng. Streaming Data Processing (Xử lý dữ liệu theo luồng) Khái niệm: Là phương pháp mà dữ liệu được xử lý ngay khi nó được tạo ra hoặc thu thập, gần như real-time. Xử lý Streaming Data cho phép phản ứng nhanh chóng đối với sự kiện và thông tin mới. Ứng dụng: Phù hợp cho các ứng dụng yêu cầu thời gian phản hồi nhanh hoặc xử lý dữ liệu liên tục, như giám sát và phân tích dữ liệu thời gian thực, IoT, và xử lý sự kiện. Ví dụ: Giám sát giao thông trong thời gian thực, phát hiện gian lận tài chính ngay lập tức, hoặc phân tích cảm xúc từ bài đăng trên mạng xã hội. Có thể thấy, Batch processing thường xử lý lượng dữ liệu lớn với độ phức tạp đơn giản và độ trễ cao (do phụ thuộc vào lịch trình xử lý dữ liệu), trong khi đó, streaming processing xử lý phức tạp hơn để quản lý dữ liệu liên tục nhưng với quy mô nhỏ hơn tại mỗi thời điểm và độ trễ thấp (near realtime).\nCác công nghệ thường dùng trong Big Data Apache Hadoop Framework mã nguồn mở được viết bằng Java cho phép xử lý phân tán (distributed processing) các tập tin dữ liệu lớn trên các nhóm/cụm máy tính (clusters of computers) sử dụng các mô hình lập trình đơn giản; được thiết kế để mở rộng từ một máy chủ duy nhất sang hàng ngàn máy khác, mỗi máy cung cấp khả năng tính toán và lưu trữ cục bộ (local computation and storage). Đọc thêm: hadoop Về Hadoop thì chỉ cần học qua về hệ sinh thái cơ bản Hadoop (HDFS, MapRedure, YARN), không cần quá deep dive.\nApache Spark Framework mã nguồn mở, dành cho việc xử lý và phân tích dữ liệu lớn (Big Data). Được thiết kế để nhanh chóng xử lý dữ liệu lớn thông qua việc thực hiện tính toán in-memory (trên RAM), với tốc độ xử lý dữ liệu có thể nhanh hơn 100 lần so với MapReduce. và có thể chạy trên cả cơ sở hạ tầng on-premise lẫn các dịch vụ on-cloud. Không có hệ thống file của riêng mình (không phụ thuộc vào bất cứ một hệ thống file nào), nó sử dụng hệ thống file khác như: HDFS, Cassandra, S3,…. Hỗ trợ nhiều kiểu định dạng file khác nhau (text, csv, json…) Xử lý dữ liệu: theo lô (batch data) và theo thời gian thực (streaming data). Hỗ trợ ngôn ngữ: Java, Scala, Python và R. Đọc thêm: spark Apache Kafka Nền tảng mã nguồn mở được sử dụng cho việc xử lý luồng dữ liệu realtime, phát triển bởi LinkedIn và hiện tại được quản lý bởi Apache Software Foundation. Trong ngữ cảnh của Big Data, Kafka được biết đến như một hệ thống truyền tải dữ liệu hiệu suất cao, đáng tin cậy, có khả năng mở rộng và linh hoạt, thích hợp cho việc xây dựng các ứng dụng xử lý dữ liệu thời gian thực. Ứng dụng: Xử lý dữ liệu thời gian thực, ghi lại log/event, truyền thông giữa các dịch vụ, tích hợp dữ liệu và ETL dữ liệu realtime,\u0026hellip; Đọc thêm: kafka Ví dụ - Xét ví dụ đối với xử lý on-prem: Về ETL dữ liệu lớn realtime có sử dụng các công nghệ thường dùng dành cho BigData như Apache Hadoop, Apache Spark, Apache Kafka như sau:\nExtract: Dùng Kafka cho việc thu thập/trích xuất dữ liệu realtime Transform: Dùng Spark Streaming để xử lý dữ liệu từ Kafka. Các dữ liệu sẽ được làm sạch, biến đổi (ví dụ: lọc bỏ các giá trị trống) và chuẩn bị cho việc phân tích. Biến đổi dữ liệu: Dữ liệu sau khi được làm sạch có thể được tổng hợp (ví dụ: tính tổng, trung bình theo các tiêu chí nào đó) và chuẩn bị cho việc lưu trữ. Load: Lưu trữ dữ liệu sau khi dữ liệu đã được xử lý bởi Spark sẽ được lưu trữ trong Hadoop HDFS, cho phép lưu trữ dữ liệu lớn một cách hiệu quả và kinh tế. Lưu trữ dữ liệu phân tích: Dữ liệu tổng hợp có thể được lưu trữ dưới dạng tệp Parquet hoặc ORC, tối ưu hóa cho việc truy vấn phân tích. Kết thúc quá trình ETL dữ liệu, bạn sẽ thành công trích xuất dữ liệu realtime, và thực hiện biến đổi dữ liệu rồi lưu vào hệ thống lưu trữ đích (ở ví dụ trên thì là hệ lưu trữ và quản lý dữ liệu phân tán - HDFS) "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]